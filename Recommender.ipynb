{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system for Kickstarter\n",
    "\n",
    "Like many other vendor web applications (though \"vendor\" here isn't completely accurate, as Kickstarter itself isn't selling products), Kickstarter has many products that a user (\"backer\") may sift through and possibly buy (more accurately, \"back\" or \"give money to\").\n",
    "\n",
    "It's in the interest of Kickstarter to advertise as many products as possible to a potential backer because it makes 5% of the money that is funded for a product. If a backer only sees a limited number of products, then Kickstarter may not earn potentially as much money because it would be difficult for backers to find other products they are interested in. In order to lower that difficulty, Kickstarter can recommend products based on what a user has clicked on in the past, or, given a page that a person is looking at, recommend products that are similar to that.\n",
    "\n",
    "# How I'll approach building the recommendation system\n",
    "\n",
    "The Kickstarter website does not have an API, though web scraping is a \"trivial\" task that can allow me to access public information such as the Kickstarter project title or the amount needed for the project, just to name a few. Here, I had actually taken some data that was pre-scraped, but scraping would involve grabbing data with a package like Python's ```MechanicalSoup``` and searching the data to find data such as the project's ```name```, ```category```, or ```description```.\n",
    "\n",
    "In principle I would obtain data from all of those fields (and whatever else I can find) and split up the data into something easily processed by a computer with a text analyzer, but in order to keep the amount that I need to process relatively low, I'll only consider the ```name```, ```category```, and ```main_category```. The main idea here is that I will vectorize each project and measure how close each project is to each other via an inner product. Roughly speaking, I'll be creating recommendations by analyzing how often there are matches in those fields.\n",
    "\n",
    "To make this a bit more concrete, suppose that there are two products whose ```main_category``` is food, but the ```category``` of one is \"Restaurants\" and the ```category``` of the other one is \"Drinks\", these two products are not exact matches, but still match up relatively well because of the match on ```main_category```. Suppose that the ```name``` of the \"Restaurants\" product is \"Monarch Espresso Bar\" and the ```name``` of the other product is \"\"Espresso Machine with Bean Grinder,\" you would expect that those two products are related because of the match in the ```name``` field on the word \"espresso\".\n",
    "\n",
    "Ultimately, this should work better if I could include more data from the ```description``` field of each product, but again, in the interest of making a product that is a proof-of-concept and in the interest of not using too much processing power and memory, I'll only look at the fields I mentioned a couple paragraphs ago.\n",
    "\n",
    "# Selecting the data\n",
    "\n",
    "I'm not necessarily trying to predict general features from a small sample space; I will actually just take the entire population as if I were trying explore the data a bit and only select the fields I'm interested in (```name```, ```category```, and ```main_category```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fields = ['name','category','main_category']\n",
    "\n",
    "df1 = pd.read_csv('ks-projects-201612.csv', sep=',', header=0, encoding='latin1', usecols=fields)\n",
    "df2 = pd.read_csv('ks-projects-201801.csv', sep=',', header=0, encoding='latin1', usecols=fields)\n",
    "\n",
    "df = pd.concat([df1,df2], ignore_index=True)\n",
    "df_original = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data here is actually a bit corrupted and I'd rather not manually put quotation marks around the items that ought to be considered as ```name```. There are not that many, but with the number of rows being on the order of 100s of thousands, it'd require a certain type of person to fix those (and as of writing this I have an idea on how to fix it, but I'd like to get the basic idea down first). If I drop the weirdly-categorized entries, I won't lose much. In fact, there's only a few hundred of those entries.\n",
    "\n",
    "Down here I had to fiddle around with the filtering because there were small coincidences with some names having the same string spilling over into the ```category``` field. I didn't show the code below, but I had output the categories into a text file and did a quick check by eye to make sure nothing suspicious was in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories_with_unique_values = df.groupby(['category']).count()['name'] <= 7\n",
    "\n",
    "good_categories = categories_with_unique_values[categories_with_unique_values == False].index\n",
    "\n",
    "df = df[df['category'].map(lambda x: x in good_categories)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from fixing the categories, I want to begin normalizing the text data so that I can vectorize it. There are a few techniques that I'd like to use here:\n",
    "\n",
    "* Stemming\n",
    "\n",
    "* Normalizing the case of the letter\n",
    "\n",
    "* Taking out stopwords\n",
    "\n",
    "This only really applies to the ```name``` feature, as the space of ```category``` and ```main_category``` are just made up of discrete, pre-determined labels. In the stop words, you'll notice that I add in some extra words. This is from an early analysis of the most common words. Words like \"project\" and \"cancel\" can unintentionally link unrelated projects (e.g. \"Project for a surrealist film\" or \"A dungeons & dragons project for kids\". N.b. these aren't actual names but something I made up on the spot to demonstrate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jeff\\documents\\data_sci\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\jeff\\documents\\data_sci\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\jeff\\documents\\data_sci\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "df['name'] = df['name'].str.lower().astype(str)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_stop_words = set(['project','cancel','one'])\n",
    "stop_words = stop_words.union(extra_stop_words)\n",
    "\n",
    "df['name'] = df['name'].str.split(' ').apply(lambda x: ' '.join(w for w in x if w not in stop_words))\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "df['name'] = df['name'].str.replace('[{}]'.format(punctuation), '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the vector space isn't too large, I'll cull some words by trying to find the roots. There are a couple methods to attempt this: lemmatizing and stemming. To summarize what these do:\n",
    "\n",
    "* Stemmers more or less do a crude chopping of the end of a word with the hope that it can find the root word or something close to it.\n",
    "\n",
    "* Lemmatizers try to find actual words by using the morphological rules of a language.\n",
    "\n",
    "Stemmers are generally quicker because their algorithm is more or less simply chopping off part of a word, and lemmatizers have to go through some set of rules to try and morph a word back into its root form. In some cases, lemmatizers don't do their job properly (I used ```nltk```'s ```WordNetLemmatizer``` to try and find the root word of \"chatting,\" but it did not return \"chat\" like expected; ```SnowballStemmer``` had done that, and with a much faster time). I'd like the recommender system to perform a bit quickly and I actually do not care what the true root of a word is, so I'll stick with a stemmer. The reason why I do not particularly care about the true root of the word is that even if a stemmer cuts a certain class of words down to something that isn't exactly the root word, but still comes close, as long as I can recognize that part of the root word is the same among stemmed words, then I can group them together (e.g. perhaps \"saves\" and \"saving\" will be stemmed down to \"sav,\" but I would recognize that they come from the same word, so the vector that originally had \"saves\" and the vector that originally had \"saving\" can have an inner product which is non-zero). In particular, the ```SnowballStemmer``` (also known as the \"Porter2\" stemmer) is generally considered to be a compromise between a more relaxed Porter stemmer and an aggressive Lancaster stemmer \\[1\\].\n",
    "\n",
    "\\[1\\] [Answer to differences between Porter and Lancaster stemming. This answer fails to give further citations, however.](https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jeff\\documents\\data_sci\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "df['name'] = df['name'].str.split(' ').apply(lambda x: ' '.join(stemmer.stem(w) for w in x))\n",
    "\n",
    "text = df['name'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jeff\\documents\\data_sci\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['name_and_categories'] = df['name'] + ' ' + df['category'] + ' ' + df['main_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to take a look at a word cloud to make sure that there are, in fact, a set of words in the total space of text that have relative amounts. That way, when I consider the inner product between two vectors, it is possible that there are some measurable differences. Put more simply: suppose that all the text was such that each word appears only once, then it's actually a bit pointless to use the ```name``` feature because there is no overlap between each ```name```, which is to say in the language of linear algebra, each vector with elements strictly in ```name``` would be orthogonal.\n",
    "\n",
    "Basically, all I need to do is examine the frequency of a word in the corpus to see how common it is. Word clouds are an easy way to visualize this. For this part, I'm using Andreas Mueller's [post on creating a word cloud in Python](https://github.com/amueller/word_cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(min_df=0, max_features=100)\n",
    "counts = cv.fit_transform([text]).toarray().ravel()\n",
    "words = np.array(cv.get_feature_names())\n",
    "\n",
    "counts = counts / float(counts.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=50).generate(text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think \"cancel\" is a word that might accidentally cause interactions between unrelated projects. I thought I had cancelled it above, but perhaps I need to filter it after the stemming. In any case, this seems like a good starting point. Hopefully with the more dominant words in the word cloud I can get some on-point recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer().fit_transform(df['name'])\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NERO: RFID Blocking Wallet Leather or Kevlar + Carbon Fiber\n",
      "Design\n",
      "\n",
      "nero rfid blocking wallet leather kevlar  carbon fiber\n",
      "nero rfid blocking wallet leather kevlar  carbon fiber\n",
      "carbon fiber  kevlar wallet\n",
      "carbon fiber  kevlar wallet\n",
      "carbon fiber wallet\n",
      "carbon fiber wallet\n",
      "ultimate rfid blocking minimalist wallet\n",
      "ultimate rfid blocking minimalist wallet\n",
      "w1 wallet rfid blocking\n"
     ]
    }
   ],
   "source": [
    "given_index = 1000\n",
    "\n",
    "name_of_item = df.loc[given_index, 'name']\n",
    "main_category = df.loc[given_index, 'main_category']\n",
    "main_category_df = df[df['main_category'] == main_category].reset_index().drop(columns='index')\n",
    "item_index = main_category_df[main_category_df['name'] == name_of_item].index[0]\n",
    "\n",
    "tv = TfidfVectorizer(analyzer='word').fit_transform(main_category_df['name'])\n",
    "\n",
    "cosine_similarities = linear_kernel(tv[item_index], tv).flatten()\n",
    "related_projects_indices = cosine_similarities.argsort()[:-10:-1]\n",
    "related_projects_scores = cosine_similarities[related_projects_indices]\n",
    "\n",
    "print(df_original.loc[given_index, 'name'])\n",
    "print(df_original.loc[given_index, 'main_category'])\n",
    "print()\n",
    "\n",
    "for index in related_projects_indices:\n",
    "    print(str(main_category_df.loc[index, 'name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good/interesting ones:\n",
    "\n",
    "* \\#20 (Food) Mountain brew\n",
    "\n",
    "* \\#60 (Crafts) \"Flying\" Carpets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something weird here: A lot of the foods seem to attract beers (cf. #101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
